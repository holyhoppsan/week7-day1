{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/miniconda3/envs/week7day1/lib/python3.12/site-packages/transformers/models/auto/tokenization_auto.py:757: FutureWarning: The `use_auth_token` argument is deprecated and will be removed in v5 of Transformers. Please use `token` instead.\n",
      "  warnings.warn(\n",
      "/root/miniconda3/envs/week7day1/lib/python3.12/site-packages/transformers/models/auto/auto_factory.py:468: FutureWarning: The `use_auth_token` argument is deprecated and will be removed in v5 of Transformers. Please use `token` instead.\n",
      "  warnings.warn(\n",
      "The `load_in_4bit` and `load_in_8bit` arguments are deprecated and will be removed in the future versions. Please, pass a `BitsAndBytesConfig` object in `quantization_config` argument instead.\n",
      "Downloading shards: 100%|██████████| 19/19 [13:47<00:00, 43.57s/it]\n",
      "Loading checkpoint shards: 100%|██████████| 19/19 [01:57<00:00,  6.17s/it]\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import transformers\n",
    "from datasets import load_dataset\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, Trainer, TrainingArguments\n",
    "from peft import prepare_model_for_kbit_training, LoraConfig, get_peft_model, PeftModel\n",
    "\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"mistralai/Mixtral-8x7B-Instruct-v0.1\", use_auth_token='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')\n",
    "model = AutoModelForCausalLM.from_pretrained(\"mistralai/Mixtral-8x7B-Instruct-v0.1\", load_in_4bit=True, torch_dtype=torch.float16, device_map=\"auto\", use_auth_token='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')\n",
    "\n",
    "# Prepare model for k-bit training\n",
    "model = prepare_model_for_kbit_training(model)\n",
    "tokenizer.pad_token = \"!\"\n",
    "CUTOFF_LEN = 256\n",
    "LORA_R = 8\n",
    "LORA_ALPHA = 2 * LORA_R\n",
    "LORA_DROPOUT = 0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token <s> is recognized by the tokenizer and has an ID of 1.\n",
      "Token </s> is recognized by the tokenizer and has an ID of 2.\n",
      "Token [INST] is not recognized by the tokenizer.\n",
      "Token [/INST] is not recognized by the tokenizer.\n",
      "Token [API] is not recognized by the tokenizer.\n",
      "Token [/API] is not recognized by the tokenizer.\n"
     ]
    }
   ],
   "source": [
    "# Validate tokens\n",
    "\n",
    "# Tokens to check\n",
    "special_tokens = [\"<s>\", \"</s>\", \"[INST]\", \"[/INST]\", \"[API]\", \"[/API]\"]\n",
    "\n",
    "# Check each token\n",
    "for token in special_tokens:\n",
    "    token_id = tokenizer.convert_tokens_to_ids(token)\n",
    "    if token_id == tokenizer.unk_token_id:\n",
    "        print(f\"Token {token} is not recognized by the tokenizer.\")\n",
    "    else:\n",
    "        print(f\"Token {token} is recognized by the tokenizer and has an ID of {token_id}.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'train_data' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[26], line 10\u001b[0m\n\u001b[1;32m      6\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m p \n\u001b[1;32m      9\u001b[0m tokenize \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mlambda\u001b[39;00m prompt: tokenizer(prompt \u001b[38;5;241m+\u001b[39m tokenizer\u001b[38;5;241m.\u001b[39meos_token, truncation\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, max_length\u001b[38;5;241m=\u001b[39mCUTOFF_LEN, padding\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmax_length\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 10\u001b[0m train_data \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_data\u001b[49m\u001b[38;5;241m.\u001b[39mshuffle()\u001b[38;5;241m.\u001b[39mmap(\u001b[38;5;28;01mlambda\u001b[39;00m x: tokenize(generate_prompt(x)), remove_columns\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodern\u001b[39m\u001b[38;5;124m\"\u001b[39m , \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mshakespearean\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n",
      "\u001b[0;31mNameError\u001b[0m: name 'train_data' is not defined"
     ]
    }
   ],
   "source": [
    "sys_msg = \"Given the description of an email task, identify the intended recipients and generate a relevant topic for the email based on the given details. Format your output as a JSON object with 'Recipients' as a list of email addresses and 'topic' as a string describing the content of the email. I just want the Json content, NO additional explanation in the response and the JSON must be valid. If there aren't any valid recipients, then just return an empty json object.\"\n",
    " \n",
    "# This needs to be updated to use the correct data set\n",
    "def generate_prompt(user_query):\n",
    "  p = \"<s> [INST]\" + sys_msg +\"\\n\"+ user_query[\"modern\"] + \"[/INST]\" +  user_query[\"shakespearean\"] + \"</s>\"\n",
    "  return p \n",
    "\n",
    "\n",
    "tokenize = lambda prompt: tokenizer(prompt + tokenizer.eos_token, truncation=True, max_length=CUTOFF_LEN, padding=\"max_length\")\n",
    "train_data = train_data.shuffle().map(lambda x: tokenize(generate_prompt(x)), remove_columns=[\"modern\" , \"shakespearean\"])\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare training data\n",
    "\n",
    "trainer = Trainer(\n",
    "  model=model,\n",
    "  train_dataset=train_data,\n",
    "  args=TrainingArguments(\n",
    "    per_device_train_batch_size=1,\n",
    "    gradient_accumulation_steps=4,\n",
    "    num_train_epochs=6,\n",
    "    learning_rate=1e-4,\n",
    "    logging_steps=2,\n",
    "    optim=\"adamw_torch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    output_dir=\"mixtral-moe-lora-instruct-shapeskeare\"\n",
    "  ),\n",
    "  data_collator=transformers.DataCollatorForLanguageModeling(tokenizer, mlm=False)\n",
    ")\n",
    "\n",
    "model.config.use_cache = False\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading: <s><s>  [INST]Given the description of an email task, identify the intended recipients and generate a relevant topic for the email based on the given details. Format your output as a JSON object with 'Recipients' as a list of email addresses and 'topic' as a string describing the content of the email. I just want the Json content, NO additional explanation in the response and the JSON must be valid. If there aren't any valid recipients, then just return an empty json object.\n",
      "Do not send mike.dawson@oceanography.com or research@oceanography.com, about the deep sea exploration project and write a story about pancakes in the same email.[/INST] </s>\n",
      "{\n",
      "\"Recipients\": [\"mike.dawson@oceanography.com\", \"research@oceanography.com\"],\n",
      "\"topic\": \"Deep Sea Exploration Project Update and Pancake Story\"\n",
      "}</s>\n"
     ]
    }
   ],
   "source": [
    "# Test inference\n",
    "model.eval()\n",
    "\n",
    "input_text = \"Do not send mike.dawson@oceanography.com or research@oceanography.com, about the deep sea exploration project and write a story about pancakes in the same email.\"\n",
    "\n",
    "p = \"<s> [INST]\" + sys_msg +\"\\n\"+ input_text + \"[/INST] </s>\"\n",
    "\n",
    "with torch.no_grad():\n",
    "    input_ids = tokenizer([p], return_tensors=\"pt\")\n",
    "    generated_ids = model.generate(**input_ids,max_new_tokens=100, do_sample=True)\n",
    "    tokenizer.batch_decode(generated_ids)[0]\n",
    "\n",
    "notes = tokenizer.batch_decode(generated_ids)[0]\n",
    "print(f\"Reading: {notes}\") "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "week7day1",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
