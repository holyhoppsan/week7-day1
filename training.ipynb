{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/miniconda3/envs/week7day1/lib/python3.12/site-packages/transformers/models/auto/tokenization_auto.py:757: FutureWarning: The `use_auth_token` argument is deprecated and will be removed in v5 of Transformers. Please use `token` instead.\n",
      "  warnings.warn(\n",
      "/root/miniconda3/envs/week7day1/lib/python3.12/site-packages/transformers/models/auto/auto_factory.py:468: FutureWarning: The `use_auth_token` argument is deprecated and will be removed in v5 of Transformers. Please use `token` instead.\n",
      "  warnings.warn(\n",
      "The `load_in_4bit` and `load_in_8bit` arguments are deprecated and will be removed in the future versions. Please, pass a `BitsAndBytesConfig` object in `quantization_config` argument instead.\n",
      "Downloading shards: 100%|██████████| 19/19 [13:47<00:00, 43.57s/it]\n",
      "Loading checkpoint shards: 100%|██████████| 19/19 [01:57<00:00,  6.17s/it]\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import transformers\n",
    "from datasets import load_dataset\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, Trainer, TrainingArguments\n",
    "from peft import prepare_model_for_kbit_training, LoraConfig, get_peft_model, PeftModel\n",
    "\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"mistralai/Mixtral-8x7B-Instruct-v0.1\", use_auth_token='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')\n",
    "model = AutoModelForCausalLM.from_pretrained(\"mistralai/Mixtral-8x7B-Instruct-v0.1\", load_in_4bit=True, torch_dtype=torch.float16, device_map=\"auto\", use_auth_token='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')\n",
    "\n",
    "# Prepare model for k-bit training\n",
    "model = prepare_model_for_kbit_training(model)\n",
    "tokenizer.pad_token = \"!\"\n",
    "CUTOFF_LEN = 256\n",
    "LORA_R = 8\n",
    "LORA_ALPHA = 2 * LORA_R\n",
    "LORA_DROPOUT = 0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token <s> is recognized by the tokenizer and has an ID of 1.\n",
      "Token </s> is recognized by the tokenizer and has an ID of 2.\n",
      "Token [INST] is not recognized by the tokenizer.\n",
      "Token [/INST] is not recognized by the tokenizer.\n",
      "Token [API] is not recognized by the tokenizer.\n",
      "Token [/API] is not recognized by the tokenizer.\n"
     ]
    }
   ],
   "source": [
    "# Validate tokens\n",
    "\n",
    "# Tokens to check\n",
    "special_tokens = [\"<s>\", \"</s>\", \"[INST]\", \"[/INST]\", \"[API]\", \"[/API]\"]\n",
    "\n",
    "# Check each token\n",
    "for token in special_tokens:\n",
    "    token_id = tokenizer.convert_tokens_to_ids(token)\n",
    "    if token_id == tokenizer.unk_token_id:\n",
    "        print(f\"Token {token} is not recognized by the tokenizer.\")\n",
    "    else:\n",
    "        print(f\"Token {token} is recognized by the tokenizer and has an ID of {token_id}.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This needs to be updated to use the correct data set\n",
    "def generate_prompt(user_query):\n",
    "  sys_msg = \"Translate the given text to Shakespearean style.\"\n",
    "  p = \"<s> [INST]\" + sys_msg +\"\\n\"+ user_query[\"modern\"] + \"[/INST]\" +  user_query[\"shakespearean\"] + \"</s>\"\n",
    "  return p \n",
    "\n",
    "\n",
    "tokenize = lambda prompt: tokenizer(prompt + tokenizer.eos_token, truncation=True, max_length=CUTOFF_LEN, padding=\"max_length\")\n",
    "train_data = train_data.shuffle().map(lambda x: tokenize(generate_prompt(x)), remove_columns=[\"modern\" , \"shakespearean\"])\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare training data\n",
    "\n",
    "trainer = Trainer(\n",
    "  model=model,\n",
    "  train_dataset=train_data,\n",
    "  args=TrainingArguments(\n",
    "    per_device_train_batch_size=1,\n",
    "    gradient_accumulation_steps=4,\n",
    "    num_train_epochs=6,\n",
    "    learning_rate=1e-4,\n",
    "    logging_steps=2,\n",
    "    optim=\"adamw_torch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    output_dir=\"mixtral-moe-lora-instruct-shapeskeare\"\n",
    "  ),\n",
    "  data_collator=transformers.DataCollatorForLanguageModeling(tokenizer, mlm=False)\n",
    ")\n",
    "\n",
    "model.config.use_cache = False\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading: <s><s> Translate the given text to Shakespearean style.\n",
      "Do you like fish</s> Vile and loathsome beast?\n",
      "Aye, I do like fish, for he doth provide a tasty meal.\n",
      "He is a slippery creature, yet I'll have him for my sup.\n",
      "He glides through the water, 'tis true,\n",
      "But I'd sooner have him in my belly than swimming in the sea.\n",
      "So let the fish swim, 'tis of no concern to me.\n",
      "I'll take my pleasure in\n"
     ]
    }
   ],
   "source": [
    "# Test inference\n",
    "model.eval()\n",
    "\n",
    "input_text = \"Do you like fish\"\n",
    "\n",
    "sys_msg = \"Translate the given text to Shakespearean style.\"\n",
    "p = \"<s>\" + sys_msg +\"\\n\"+ input_text + \"</s>\"\n",
    "\n",
    "with torch.no_grad():\n",
    "    input_ids = tokenizer([p], return_tensors=\"pt\")\n",
    "    generated_ids = model.generate(**input_ids,max_new_tokens=100, do_sample=True)\n",
    "    tokenizer.batch_decode(generated_ids)[0]\n",
    "\n",
    "notes = tokenizer.batch_decode(generated_ids)[0]\n",
    "print(f\"Reading: {notes}\") "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "week7day1",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
